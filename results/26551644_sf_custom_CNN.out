lr: 5e-06
weight_decay: 0.0001
factor: 0.3
patience: 5
n_epochs: 500
The code will run on GPU.
Loss train: 2.310	 val: 2.295	 Accuracy train: 10.8%	 val: 11.7%
Validation loss improved from 10000000000.0000 to 2.2945. Saving model...
Loss train: 2.289	 val: 2.284	 Accuracy train: 14.0%	 val: 16.7%
Validation loss improved from 2.2945 to 2.2836. Saving model...
Loss train: 2.270	 val: 2.274	 Accuracy train: 18.8%	 val: 16.7%
Validation loss improved from 2.2836 to 2.2738. Saving model...
Loss train: 2.255	 val: 2.265	 Accuracy train: 20.0%	 val: 18.3%
Validation loss improved from 2.2738 to 2.2648. Saving model...
Loss train: 2.239	 val: 2.255	 Accuracy train: 22.0%	 val: 18.3%
Validation loss improved from 2.2648 to 2.2548. Saving model...
Loss train: 2.223	 val: 2.246	 Accuracy train: 23.8%	 val: 19.2%
Validation loss improved from 2.2548 to 2.2457. Saving model...
Loss train: 2.207	 val: 2.235	 Accuracy train: 28.0%	 val: 19.2%
Validation loss improved from 2.2457 to 2.2353. Saving model...
Loss train: 2.192	 val: 2.227	 Accuracy train: 29.2%	 val: 19.2%
Validation loss improved from 2.2353 to 2.2266. Saving model...
Loss train: 2.176	 val: 2.216	 Accuracy train: 30.2%	 val: 19.2%
Validation loss improved from 2.2266 to 2.2160. Saving model...
Loss train: 2.161	 val: 2.206	 Accuracy train: 30.8%	 val: 20.0%
Validation loss improved from 2.2160 to 2.2063. Saving model...
Loss train: 2.144	 val: 2.196	 Accuracy train: 32.2%	 val: 20.8%
Validation loss improved from 2.2063 to 2.1961. Saving model...
Loss train: 2.128	 val: 2.187	 Accuracy train: 34.0%	 val: 23.3%
Validation loss improved from 2.1961 to 2.1866. Saving model...
Loss train: 2.112	 val: 2.178	 Accuracy train: 35.0%	 val: 22.5%
Validation loss improved from 2.1866 to 2.1783. Saving model...
Loss train: 2.097	 val: 2.169	 Accuracy train: 36.4%	 val: 25.8%
Validation loss improved from 2.1783 to 2.1693. Saving model...
Loss train: 2.079	 val: 2.159	 Accuracy train: 40.2%	 val: 26.7%
Validation loss improved from 2.1693 to 2.1594. Saving model...
Loss train: 2.063	 val: 2.151	 Accuracy train: 43.2%	 val: 26.7%
Validation loss improved from 2.1594 to 2.1509. Saving model...
Loss train: 2.048	 val: 2.143	 Accuracy train: 43.6%	 val: 26.7%
Validation loss improved from 2.1509 to 2.1427. Saving model...
Loss train: 2.033	 val: 2.137	 Accuracy train: 44.4%	 val: 27.5%
Validation loss improved from 2.1427 to 2.1368. Saving model...
Loss train: 2.018	 val: 2.129	 Accuracy train: 45.6%	 val: 28.3%
Validation loss improved from 2.1368 to 2.1293. Saving model...
Loss train: 2.003	 val: 2.121	 Accuracy train: 45.0%	 val: 27.5%
Validation loss improved from 2.1293 to 2.1214. Saving model...
Loss train: 1.990	 val: 2.114	 Accuracy train: 45.4%	 val: 28.3%
Validation loss improved from 2.1214 to 2.1141. Saving model...
Loss train: 1.975	 val: 2.107	 Accuracy train: 45.8%	 val: 28.3%
Validation loss improved from 2.1141 to 2.1070. Saving model...
Loss train: 1.961	 val: 2.100	 Accuracy train: 46.6%	 val: 31.7%
Validation loss improved from 2.1070 to 2.0999. Saving model...
Loss train: 1.947	 val: 2.093	 Accuracy train: 47.0%	 val: 33.3%
Validation loss improved from 2.0999 to 2.0929. Saving model...
Loss train: 1.932	 val: 2.086	 Accuracy train: 48.6%	 val: 35.0%
Validation loss improved from 2.0929 to 2.0856. Saving model...
Loss train: 1.919	 val: 2.078	 Accuracy train: 49.4%	 val: 35.0%
Validation loss improved from 2.0856 to 2.0785. Saving model...
Loss train: 1.905	 val: 2.071	 Accuracy train: 49.8%	 val: 34.2%
Validation loss improved from 2.0785 to 2.0711. Saving model...
Loss train: 1.891	 val: 2.063	 Accuracy train: 50.0%	 val: 33.3%
Validation loss improved from 2.0711 to 2.0632. Saving model...
Loss train: 1.877	 val: 2.056	 Accuracy train: 51.4%	 val: 34.2%
Validation loss improved from 2.0632 to 2.0559. Saving model...
Loss train: 1.864	 val: 2.049	 Accuracy train: 52.2%	 val: 35.0%
Validation loss improved from 2.0559 to 2.0486. Saving model...
Loss train: 1.851	 val: 2.041	 Accuracy train: 52.8%	 val: 36.7%
Validation loss improved from 2.0486 to 2.0412. Saving model...
Loss train: 1.838	 val: 2.034	 Accuracy train: 53.4%	 val: 38.3%
Validation loss improved from 2.0412 to 2.0339. Saving model...
Loss train: 1.825	 val: 2.026	 Accuracy train: 53.8%	 val: 38.3%
Validation loss improved from 2.0339 to 2.0264. Saving model...
Loss train: 1.813	 val: 2.020	 Accuracy train: 55.2%	 val: 38.3%
Validation loss improved from 2.0264 to 2.0196. Saving model...
Loss train: 1.800	 val: 2.012	 Accuracy train: 56.0%	 val: 37.5%
Validation loss improved from 2.0196 to 2.0124. Saving model...
Loss train: 1.787	 val: 2.005	 Accuracy train: 56.8%	 val: 37.5%
Validation loss improved from 2.0124 to 2.0049. Saving model...
Loss train: 1.774	 val: 1.997	 Accuracy train: 57.2%	 val: 38.3%
Validation loss improved from 2.0049 to 1.9973. Saving model...
Loss train: 1.762	 val: 1.990	 Accuracy train: 57.2%	 val: 38.3%
Validation loss improved from 1.9973 to 1.9900. Saving model...
Loss train: 1.748	 val: 1.984	 Accuracy train: 57.4%	 val: 37.5%
Validation loss improved from 1.9900 to 1.9836. Saving model...
Loss train: 1.737	 val: 1.976	 Accuracy train: 57.8%	 val: 37.5%
Validation loss improved from 1.9836 to 1.9763. Saving model...
Loss train: 1.724	 val: 1.968	 Accuracy train: 58.4%	 val: 39.2%
Validation loss improved from 1.9763 to 1.9684. Saving model...
Loss train: 1.712	 val: 1.961	 Accuracy train: 59.2%	 val: 39.2%
Validation loss improved from 1.9684 to 1.9608. Saving model...
Loss train: 1.699	 val: 1.953	 Accuracy train: 59.6%	 val: 39.2%
Validation loss improved from 1.9608 to 1.9531. Saving model...
Loss train: 1.688	 val: 1.945	 Accuracy train: 59.8%	 val: 40.0%
Validation loss improved from 1.9531 to 1.9453. Saving model...
Loss train: 1.675	 val: 1.937	 Accuracy train: 61.0%	 val: 40.0%
Validation loss improved from 1.9453 to 1.9373. Saving model...
Loss train: 1.663	 val: 1.929	 Accuracy train: 61.6%	 val: 40.0%
Validation loss improved from 1.9373 to 1.9294. Saving model...
Loss train: 1.652	 val: 1.922	 Accuracy train: 61.4%	 val: 40.8%
Validation loss improved from 1.9294 to 1.9218. Saving model...
Loss train: 1.640	 val: 1.914	 Accuracy train: 61.8%	 val: 40.8%
Validation loss improved from 1.9218 to 1.9141. Saving model...
Loss train: 1.628	 val: 1.907	 Accuracy train: 62.0%	 val: 41.7%
Validation loss improved from 1.9141 to 1.9067. Saving model...
Loss train: 1.616	 val: 1.899	 Accuracy train: 62.2%	 val: 42.5%
Validation loss improved from 1.9067 to 1.8989. Saving model...
Loss train: 1.605	 val: 1.891	 Accuracy train: 62.8%	 val: 42.5%
Validation loss improved from 1.8989 to 1.8914. Saving model...
Loss train: 1.593	 val: 1.884	 Accuracy train: 63.0%	 val: 43.3%
Validation loss improved from 1.8914 to 1.8836. Saving model...
Loss train: 1.582	 val: 1.876	 Accuracy train: 63.4%	 val: 43.3%
Validation loss improved from 1.8836 to 1.8760. Saving model...
Loss train: 1.570	 val: 1.868	 Accuracy train: 64.0%	 val: 45.0%
Validation loss improved from 1.8760 to 1.8681. Saving model...
Loss train: 1.558	 val: 1.860	 Accuracy train: 64.4%	 val: 45.8%
Validation loss improved from 1.8681 to 1.8603. Saving model...
Loss train: 1.547	 val: 1.854	 Accuracy train: 65.8%	 val: 45.0%
Validation loss improved from 1.8603 to 1.8536. Saving model...
Loss train: 1.535	 val: 1.846	 Accuracy train: 66.0%	 val: 45.8%
Validation loss improved from 1.8536 to 1.8458. Saving model...
Loss train: 1.523	 val: 1.838	 Accuracy train: 67.2%	 val: 46.7%
Validation loss improved from 1.8458 to 1.8378. Saving model...
Loss train: 1.512	 val: 1.830	 Accuracy train: 67.4%	 val: 47.5%
Validation loss improved from 1.8378 to 1.8298. Saving model...
Loss train: 1.501	 val: 1.822	 Accuracy train: 67.8%	 val: 47.5%
Validation loss improved from 1.8298 to 1.8219. Saving model...
Loss train: 1.489	 val: 1.814	 Accuracy train: 68.2%	 val: 47.5%
Validation loss improved from 1.8219 to 1.8137. Saving model...
Loss train: 1.478	 val: 1.806	 Accuracy train: 68.6%	 val: 48.3%
Validation loss improved from 1.8137 to 1.8062. Saving model...
Loss train: 1.467	 val: 1.799	 Accuracy train: 68.8%	 val: 48.3%
Validation loss improved from 1.8062 to 1.7988. Saving model...
Loss train: 1.456	 val: 1.791	 Accuracy train: 69.0%	 val: 48.3%
Validation loss improved from 1.7988 to 1.7911. Saving model...
Loss train: 1.445	 val: 1.784	 Accuracy train: 69.2%	 val: 48.3%
Validation loss improved from 1.7911 to 1.7837. Saving model...
Loss train: 1.434	 val: 1.776	 Accuracy train: 69.4%	 val: 48.3%
Validation loss improved from 1.7837 to 1.7761. Saving model...
Loss train: 1.423	 val: 1.770	 Accuracy train: 69.6%	 val: 48.3%
Validation loss improved from 1.7761 to 1.7698. Saving model...
Loss train: 1.413	 val: 1.761	 Accuracy train: 70.4%	 val: 48.3%
Validation loss improved from 1.7698 to 1.7613. Saving model...
Loss train: 1.402	 val: 1.754	 Accuracy train: 70.8%	 val: 48.3%
Validation loss improved from 1.7613 to 1.7539. Saving model...
Loss train: 1.392	 val: 1.746	 Accuracy train: 70.8%	 val: 49.2%
Validation loss improved from 1.7539 to 1.7463. Saving model...
Loss train: 1.381	 val: 1.739	 Accuracy train: 71.2%	 val: 49.2%
Validation loss improved from 1.7463 to 1.7390. Saving model...
Loss train: 1.371	 val: 1.732	 Accuracy train: 71.6%	 val: 50.8%
Validation loss improved from 1.7390 to 1.7316. Saving model...
Loss train: 1.360	 val: 1.725	 Accuracy train: 72.4%	 val: 51.7%
Validation loss improved from 1.7316 to 1.7246. Saving model...
Loss train: 1.350	 val: 1.717	 Accuracy train: 73.6%	 val: 50.8%
Validation loss improved from 1.7246 to 1.7173. Saving model...
Loss train: 1.340	 val: 1.710	 Accuracy train: 74.6%	 val: 51.7%
Validation loss improved from 1.7173 to 1.7104. Saving model...
Loss train: 1.330	 val: 1.703	 Accuracy train: 76.0%	 val: 51.7%
Validation loss improved from 1.7104 to 1.7032. Saving model...
Loss train: 1.320	 val: 1.698	 Accuracy train: 76.4%	 val: 51.7%
Validation loss improved from 1.7032 to 1.6978. Saving model...
Loss train: 1.310	 val: 1.689	 Accuracy train: 77.0%	 val: 52.5%
Validation loss improved from 1.6978 to 1.6893. Saving model...
Loss train: 1.300	 val: 1.683	 Accuracy train: 77.6%	 val: 52.5%
Validation loss improved from 1.6893 to 1.6826. Saving model...
Loss train: 1.290	 val: 1.676	 Accuracy train: 77.6%	 val: 53.3%
Validation loss improved from 1.6826 to 1.6756. Saving model...
Loss train: 1.281	 val: 1.669	 Accuracy train: 77.8%	 val: 53.3%
Validation loss improved from 1.6756 to 1.6690. Saving model...
Loss train: 1.271	 val: 1.662	 Accuracy train: 78.2%	 val: 53.3%
Validation loss improved from 1.6690 to 1.6624. Saving model...
Loss train: 1.261	 val: 1.656	 Accuracy train: 78.6%	 val: 53.3%
Validation loss improved from 1.6624 to 1.6556. Saving model...
Loss train: 1.252	 val: 1.649	 Accuracy train: 79.4%	 val: 54.2%
Validation loss improved from 1.6556 to 1.6491. Saving model...
Loss train: 1.243	 val: 1.642	 Accuracy train: 80.0%	 val: 54.2%
Validation loss improved from 1.6491 to 1.6424. Saving model...
Loss train: 1.233	 val: 1.636	 Accuracy train: 80.2%	 val: 54.2%
Validation loss improved from 1.6424 to 1.6360. Saving model...
Loss train: 1.224	 val: 1.630	 Accuracy train: 80.8%	 val: 56.7%
Validation loss improved from 1.6360 to 1.6295. Saving model...
Loss train: 1.215	 val: 1.623	 Accuracy train: 81.6%	 val: 56.7%
Validation loss improved from 1.6295 to 1.6232. Saving model...
Loss train: 1.206	 val: 1.617	 Accuracy train: 81.8%	 val: 56.7%
Validation loss improved from 1.6232 to 1.6167. Saving model...
Loss train: 1.197	 val: 1.611	 Accuracy train: 82.8%	 val: 56.7%
Validation loss improved from 1.6167 to 1.6105. Saving model...
Loss train: 1.188	 val: 1.604	 Accuracy train: 83.0%	 val: 56.7%
Validation loss improved from 1.6105 to 1.6042. Saving model...
Loss train: 1.180	 val: 1.598	 Accuracy train: 83.0%	 val: 57.5%
Validation loss improved from 1.6042 to 1.5981. Saving model...
Loss train: 1.171	 val: 1.592	 Accuracy train: 83.0%	 val: 57.5%
Validation loss improved from 1.5981 to 1.5917. Saving model...
Loss train: 1.162	 val: 1.586	 Accuracy train: 83.4%	 val: 57.5%
Validation loss improved from 1.5917 to 1.5855. Saving model...
Loss train: 1.154	 val: 1.579	 Accuracy train: 84.2%	 val: 57.5%
Validation loss improved from 1.5855 to 1.5793. Saving model...
Loss train: 1.146	 val: 1.573	 Accuracy train: 84.2%	 val: 57.5%
Validation loss improved from 1.5793 to 1.5734. Saving model...
Loss train: 1.137	 val: 1.567	 Accuracy train: 84.8%	 val: 57.5%
Validation loss improved from 1.5734 to 1.5674. Saving model...
Loss train: 1.129	 val: 1.562	 Accuracy train: 85.0%	 val: 57.5%
Validation loss improved from 1.5674 to 1.5620. Saving model...
Loss train: 1.121	 val: 1.556	 Accuracy train: 85.4%	 val: 57.5%
Validation loss improved from 1.5620 to 1.5561. Saving model...
Loss train: 1.113	 val: 1.550	 Accuracy train: 86.0%	 val: 58.3%
Validation loss improved from 1.5561 to 1.5502. Saving model...
Loss train: 1.104	 val: 1.545	 Accuracy train: 86.2%	 val: 58.3%
Validation loss improved from 1.5502 to 1.5449. Saving model...
Loss train: 1.096	 val: 1.541	 Accuracy train: 86.6%	 val: 58.3%
Validation loss improved from 1.5449 to 1.5406. Saving model...
Loss train: 1.089	 val: 1.534	 Accuracy train: 86.8%	 val: 58.3%
Validation loss improved from 1.5406 to 1.5340. Saving model...
Loss train: 1.081	 val: 1.530	 Accuracy train: 87.0%	 val: 58.3%
Validation loss improved from 1.5340 to 1.5303. Saving model...
Loss train: 1.073	 val: 1.525	 Accuracy train: 87.4%	 val: 58.3%
Validation loss improved from 1.5303 to 1.5254. Saving model...
Loss train: 1.066	 val: 1.520	 Accuracy train: 87.4%	 val: 58.3%
Validation loss improved from 1.5254 to 1.5198. Saving model...
Loss train: 1.058	 val: 1.514	 Accuracy train: 88.0%	 val: 58.3%
Validation loss improved from 1.5198 to 1.5145. Saving model...
Loss train: 1.051	 val: 1.509	 Accuracy train: 88.4%	 val: 58.3%
Validation loss improved from 1.5145 to 1.5089. Saving model...
Loss train: 1.043	 val: 1.503	 Accuracy train: 88.4%	 val: 59.2%
Validation loss improved from 1.5089 to 1.5035. Saving model...
Loss train: 1.036	 val: 1.498	 Accuracy train: 88.6%	 val: 59.2%
Validation loss improved from 1.5035 to 1.4979. Saving model...
Loss train: 1.029	 val: 1.493	 Accuracy train: 88.8%	 val: 59.2%
Validation loss improved from 1.4979 to 1.4926. Saving model...
Loss train: 1.021	 val: 1.487	 Accuracy train: 88.8%	 val: 60.8%
Validation loss improved from 1.4926 to 1.4872. Saving model...
Loss train: 1.014	 val: 1.481	 Accuracy train: 89.0%	 val: 60.8%
Validation loss improved from 1.4872 to 1.4807. Saving model...
Loss train: 1.007	 val: 1.474	 Accuracy train: 89.2%	 val: 61.7%
Validation loss improved from 1.4807 to 1.4744. Saving model...
Loss train: 1.000	 val: 1.468	 Accuracy train: 89.4%	 val: 61.7%
Validation loss improved from 1.4744 to 1.4683. Saving model...
Loss train: 0.993	 val: 1.463	 Accuracy train: 89.4%	 val: 62.5%
Validation loss improved from 1.4683 to 1.4628. Saving model...
Loss train: 0.986	 val: 1.457	 Accuracy train: 89.4%	 val: 63.3%
Validation loss improved from 1.4628 to 1.4574. Saving model...
Loss train: 0.980	 val: 1.452	 Accuracy train: 89.4%	 val: 64.2%
Validation loss improved from 1.4574 to 1.4523. Saving model...
Loss train: 0.973	 val: 1.447	 Accuracy train: 89.4%	 val: 64.2%
Validation loss improved from 1.4523 to 1.4472. Saving model...
Loss train: 0.966	 val: 1.442	 Accuracy train: 89.8%	 val: 64.2%
Validation loss improved from 1.4472 to 1.4422. Saving model...
Loss train: 0.960	 val: 1.437	 Accuracy train: 89.8%	 val: 64.2%
Validation loss improved from 1.4422 to 1.4373. Saving model...
Loss train: 0.953	 val: 1.432	 Accuracy train: 89.8%	 val: 64.2%
Validation loss improved from 1.4373 to 1.4323. Saving model...
Loss train: 0.947	 val: 1.428	 Accuracy train: 89.8%	 val: 64.2%
Validation loss improved from 1.4323 to 1.4277. Saving model...
Loss train: 0.941	 val: 1.423	 Accuracy train: 89.8%	 val: 65.0%
Validation loss improved from 1.4277 to 1.4231. Saving model...
Loss train: 0.935	 val: 1.418	 Accuracy train: 90.0%	 val: 65.0%
Validation loss improved from 1.4231 to 1.4182. Saving model...
Loss train: 0.928	 val: 1.414	 Accuracy train: 90.2%	 val: 65.0%
Validation loss improved from 1.4182 to 1.4135. Saving model...
Loss train: 0.922	 val: 1.409	 Accuracy train: 90.8%	 val: 65.0%
Validation loss improved from 1.4135 to 1.4088. Saving model...
Loss train: 0.916	 val: 1.404	 Accuracy train: 90.8%	 val: 65.0%
Validation loss improved from 1.4088 to 1.4041. Saving model...
Loss train: 0.911	 val: 1.399	 Accuracy train: 91.0%	 val: 65.0%
Validation loss improved from 1.4041 to 1.3994. Saving model...
Loss train: 0.905	 val: 1.395	 Accuracy train: 91.0%	 val: 65.0%
Validation loss improved from 1.3994 to 1.3947. Saving model...
Loss train: 0.899	 val: 1.390	 Accuracy train: 91.4%	 val: 66.7%
Validation loss improved from 1.3947 to 1.3901. Saving model...
Loss train: 0.893	 val: 1.385	 Accuracy train: 91.4%	 val: 67.5%
Validation loss improved from 1.3901 to 1.3854. Saving model...
Loss train: 0.888	 val: 1.380	 Accuracy train: 91.8%	 val: 67.5%
Validation loss improved from 1.3854 to 1.3804. Saving model...
Loss train: 0.882	 val: 1.376	 Accuracy train: 92.0%	 val: 67.5%
Validation loss improved from 1.3804 to 1.3758. Saving model...
Loss train: 0.877	 val: 1.371	 Accuracy train: 92.0%	 val: 67.5%
Validation loss improved from 1.3758 to 1.3710. Saving model...
Loss train: 0.871	 val: 1.366	 Accuracy train: 92.2%	 val: 67.5%
Validation loss improved from 1.3710 to 1.3665. Saving model...
Loss train: 0.866	 val: 1.360	 Accuracy train: 92.6%	 val: 66.7%
Validation loss improved from 1.3665 to 1.3605. Saving model...
Loss train: 0.861	 val: 1.357	 Accuracy train: 92.6%	 val: 67.5%
Validation loss improved from 1.3605 to 1.3569. Saving model...
Loss train: 0.855	 val: 1.352	 Accuracy train: 92.6%	 val: 67.5%
Validation loss improved from 1.3569 to 1.3525. Saving model...
Loss train: 0.850	 val: 1.348	 Accuracy train: 93.6%	 val: 67.5%
Validation loss improved from 1.3525 to 1.3479. Saving model...
Loss train: 0.845	 val: 1.343	 Accuracy train: 93.8%	 val: 67.5%
Validation loss improved from 1.3479 to 1.3434. Saving model...
Loss train: 0.840	 val: 1.339	 Accuracy train: 93.8%	 val: 67.5%
Validation loss improved from 1.3434 to 1.3389. Saving model...
Loss train: 0.835	 val: 1.334	 Accuracy train: 93.8%	 val: 67.5%
Validation loss improved from 1.3389 to 1.3342. Saving model...
Loss train: 0.830	 val: 1.330	 Accuracy train: 93.8%	 val: 67.5%
Validation loss improved from 1.3342 to 1.3296. Saving model...
Loss train: 0.826	 val: 1.325	 Accuracy train: 93.8%	 val: 67.5%
Validation loss improved from 1.3296 to 1.3251. Saving model...
Loss train: 0.821	 val: 1.321	 Accuracy train: 93.8%	 val: 67.5%
Validation loss improved from 1.3251 to 1.3206. Saving model...
Loss train: 0.816	 val: 1.316	 Accuracy train: 93.8%	 val: 67.5%
Validation loss improved from 1.3206 to 1.3163. Saving model...
Loss train: 0.812	 val: 1.312	 Accuracy train: 94.0%	 val: 68.3%
Validation loss improved from 1.3163 to 1.3118. Saving model...
Loss train: 0.807	 val: 1.308	 Accuracy train: 94.0%	 val: 69.2%
Validation loss improved from 1.3118 to 1.3077. Saving model...
Loss train: 0.803	 val: 1.303	 Accuracy train: 94.2%	 val: 70.0%
Validation loss improved from 1.3077 to 1.3032. Saving model...
Loss train: 0.798	 val: 1.299	 Accuracy train: 94.2%	 val: 70.8%
Validation loss improved from 1.3032 to 1.2989. Saving model...
Loss train: 0.794	 val: 1.294	 Accuracy train: 94.2%	 val: 70.8%
Validation loss improved from 1.2989 to 1.2944. Saving model...
Loss train: 0.790	 val: 1.290	 Accuracy train: 94.6%	 val: 70.8%
Validation loss improved from 1.2944 to 1.2901. Saving model...
Loss train: 0.785	 val: 1.286	 Accuracy train: 94.6%	 val: 70.8%
Validation loss improved from 1.2901 to 1.2858. Saving model...
Loss train: 0.781	 val: 1.282	 Accuracy train: 94.8%	 val: 71.7%
Validation loss improved from 1.2858 to 1.2815. Saving model...
Loss train: 0.777	 val: 1.277	 Accuracy train: 94.8%	 val: 71.7%
Validation loss improved from 1.2815 to 1.2774. Saving model...
Loss train: 0.773	 val: 1.273	 Accuracy train: 95.0%	 val: 72.5%
Validation loss improved from 1.2774 to 1.2731. Saving model...
Loss train: 0.769	 val: 1.269	 Accuracy train: 95.2%	 val: 72.5%
Validation loss improved from 1.2731 to 1.2690. Saving model...
Loss train: 0.765	 val: 1.265	 Accuracy train: 95.6%	 val: 73.3%
Validation loss improved from 1.2690 to 1.2648. Saving model...
Loss train: 0.761	 val: 1.261	 Accuracy train: 95.6%	 val: 73.3%
Validation loss improved from 1.2648 to 1.2606. Saving model...
Loss train: 0.757	 val: 1.256	 Accuracy train: 95.6%	 val: 74.2%
Validation loss improved from 1.2606 to 1.2565. Saving model...
Loss train: 0.754	 val: 1.252	 Accuracy train: 95.6%	 val: 74.2%
Validation loss improved from 1.2565 to 1.2523. Saving model...
Loss train: 0.750	 val: 1.248	 Accuracy train: 95.6%	 val: 74.2%
Validation loss improved from 1.2523 to 1.2483. Saving model...
Loss train: 0.746	 val: 1.244	 Accuracy train: 96.4%	 val: 75.0%
Validation loss improved from 1.2483 to 1.2441. Saving model...
Loss train: 0.742	 val: 1.240	 Accuracy train: 96.4%	 val: 75.0%
Validation loss improved from 1.2441 to 1.2400. Saving model...
Loss train: 0.739	 val: 1.236	 Accuracy train: 97.0%	 val: 75.0%
Validation loss improved from 1.2400 to 1.2359. Saving model...
Loss train: 0.735	 val: 1.232	 Accuracy train: 97.2%	 val: 75.0%
Validation loss improved from 1.2359 to 1.2318. Saving model...
Loss train: 0.732	 val: 1.228	 Accuracy train: 97.2%	 val: 75.0%
Validation loss improved from 1.2318 to 1.2280. Saving model...
Loss train: 0.728	 val: 1.224	 Accuracy train: 97.2%	 val: 75.0%
Validation loss improved from 1.2280 to 1.2238. Saving model...
Loss train: 0.725	 val: 1.220	 Accuracy train: 97.2%	 val: 75.0%
Validation loss improved from 1.2238 to 1.2198. Saving model...
Loss train: 0.722	 val: 1.216	 Accuracy train: 97.2%	 val: 75.0%
Validation loss improved from 1.2198 to 1.2160. Saving model...
Loss train: 0.718	 val: 1.212	 Accuracy train: 97.4%	 val: 75.0%
Validation loss improved from 1.2160 to 1.2121. Saving model...
Loss train: 0.715	 val: 1.208	 Accuracy train: 98.0%	 val: 75.0%
Validation loss improved from 1.2121 to 1.2084. Saving model...
Loss train: 0.712	 val: 1.204	 Accuracy train: 98.0%	 val: 75.8%
Validation loss improved from 1.2084 to 1.2045. Saving model...
Loss train: 0.709	 val: 1.201	 Accuracy train: 98.6%	 val: 75.8%
Validation loss improved from 1.2045 to 1.2008. Saving model...
Loss train: 0.706	 val: 1.197	 Accuracy train: 98.8%	 val: 75.8%
Validation loss improved from 1.2008 to 1.1971. Saving model...
Loss train: 0.703	 val: 1.193	 Accuracy train: 98.8%	 val: 75.8%
Validation loss improved from 1.1971 to 1.1934. Saving model...
Loss train: 0.700	 val: 1.190	 Accuracy train: 98.8%	 val: 75.8%
Validation loss improved from 1.1934 to 1.1898. Saving model...
Loss train: 0.697	 val: 1.186	 Accuracy train: 98.8%	 val: 75.8%
Validation loss improved from 1.1898 to 1.1860. Saving model...
Loss train: 0.694	 val: 1.182	 Accuracy train: 99.0%	 val: 75.8%
Validation loss improved from 1.1860 to 1.1824. Saving model...
Loss train: 0.691	 val: 1.179	 Accuracy train: 99.0%	 val: 75.8%
Validation loss improved from 1.1824 to 1.1788. Saving model...
Loss train: 0.688	 val: 1.176	 Accuracy train: 99.0%	 val: 75.8%
Validation loss improved from 1.1788 to 1.1755. Saving model...
Loss train: 0.685	 val: 1.172	 Accuracy train: 99.0%	 val: 75.8%
Validation loss improved from 1.1755 to 1.1719. Saving model...
Loss train: 0.683	 val: 1.169	 Accuracy train: 99.0%	 val: 76.7%
Validation loss improved from 1.1719 to 1.1685. Saving model...
Loss train: 0.680	 val: 1.165	 Accuracy train: 99.4%	 val: 77.5%
Validation loss improved from 1.1685 to 1.1652. Saving model...
Loss train: 0.677	 val: 1.162	 Accuracy train: 99.4%	 val: 77.5%
Validation loss improved from 1.1652 to 1.1618. Saving model...
Loss train: 0.675	 val: 1.158	 Accuracy train: 99.4%	 val: 77.5%
Validation loss improved from 1.1618 to 1.1584. Saving model...
Loss train: 0.672	 val: 1.155	 Accuracy train: 99.4%	 val: 77.5%
Validation loss improved from 1.1584 to 1.1551. Saving model...
Loss train: 0.670	 val: 1.152	 Accuracy train: 99.6%	 val: 77.5%
Validation loss improved from 1.1551 to 1.1519. Saving model...
Loss train: 0.667	 val: 1.149	 Accuracy train: 99.6%	 val: 77.5%
Validation loss improved from 1.1519 to 1.1486. Saving model...
Loss train: 0.665	 val: 1.146	 Accuracy train: 99.6%	 val: 77.5%
Validation loss improved from 1.1486 to 1.1455. Saving model...
Loss train: 0.662	 val: 1.142	 Accuracy train: 99.6%	 val: 77.5%
Validation loss improved from 1.1455 to 1.1421. Saving model...
Loss train: 0.660	 val: 1.139	 Accuracy train: 99.6%	 val: 77.5%
Validation loss improved from 1.1421 to 1.1390. Saving model...
Loss train: 0.658	 val: 1.136	 Accuracy train: 99.6%	 val: 77.5%
Validation loss improved from 1.1390 to 1.1359. Saving model...
Loss train: 0.655	 val: 1.133	 Accuracy train: 99.6%	 val: 77.5%
Validation loss improved from 1.1359 to 1.1329. Saving model...
Loss train: 0.653	 val: 1.130	 Accuracy train: 99.6%	 val: 77.5%
Validation loss improved from 1.1329 to 1.1297. Saving model...
Loss train: 0.651	 val: 1.127	 Accuracy train: 99.6%	 val: 78.3%
Validation loss improved from 1.1297 to 1.1267. Saving model...
Loss train: 0.649	 val: 1.124	 Accuracy train: 99.6%	 val: 78.3%
Validation loss improved from 1.1267 to 1.1236. Saving model...
Loss train: 0.646	 val: 1.121	 Accuracy train: 99.6%	 val: 78.3%
Validation loss improved from 1.1236 to 1.1205. Saving model...
Loss train: 0.644	 val: 1.118	 Accuracy train: 99.8%	 val: 78.3%
Validation loss improved from 1.1205 to 1.1178. Saving model...
Loss train: 0.642	 val: 1.115	 Accuracy train: 99.8%	 val: 78.3%
Validation loss improved from 1.1178 to 1.1149. Saving model...
Loss train: 0.640	 val: 1.112	 Accuracy train: 99.8%	 val: 78.3%
Validation loss improved from 1.1149 to 1.1119. Saving model...
Loss train: 0.638	 val: 1.109	 Accuracy train: 99.8%	 val: 78.3%
Validation loss improved from 1.1119 to 1.1091. Saving model...
Loss train: 0.636	 val: 1.106	 Accuracy train: 99.8%	 val: 79.2%
Validation loss improved from 1.1091 to 1.1063. Saving model...
Loss train: 0.634	 val: 1.104	 Accuracy train: 99.8%	 val: 80.0%
Validation loss improved from 1.1063 to 1.1037. Saving model...
Loss train: 0.632	 val: 1.101	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.1037 to 1.1009. Saving model...
Loss train: 0.630	 val: 1.098	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.1009 to 1.0983. Saving model...
Loss train: 0.629	 val: 1.096	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0983 to 1.0956. Saving model...
Loss train: 0.627	 val: 1.093	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0956 to 1.0928. Saving model...
Loss train: 0.625	 val: 1.090	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0928 to 1.0903. Saving model...
Loss train: 0.623	 val: 1.088	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0903 to 1.0875. Saving model...
Loss train: 0.621	 val: 1.085	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0875 to 1.0851. Saving model...
Loss train: 0.620	 val: 1.083	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0851 to 1.0828. Saving model...
Loss train: 0.618	 val: 1.080	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0828 to 1.0802. Saving model...
Loss train: 0.616	 val: 1.078	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0802 to 1.0779. Saving model...
Loss train: 0.615	 val: 1.075	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0779 to 1.0755. Saving model...
Loss train: 0.613	 val: 1.073	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0755 to 1.0731. Saving model...
Loss train: 0.612	 val: 1.071	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0731 to 1.0709. Saving model...
Loss train: 0.610	 val: 1.069	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0709 to 1.0687. Saving model...
Loss train: 0.609	 val: 1.066	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0687 to 1.0664. Saving model...
Loss train: 0.607	 val: 1.064	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0664 to 1.0643. Saving model...
Loss train: 0.606	 val: 1.062	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0643 to 1.0622. Saving model...
Loss train: 0.604	 val: 1.060	 Accuracy train: 100.0%	 val: 80.8%
Validation loss improved from 1.0622 to 1.0599. Saving model...
Loss train: 0.603	 val: 1.058	 Accuracy train: 100.0%	 val: 81.7%
Validation loss improved from 1.0599 to 1.0580. Saving model...
Loss train: 0.601	 val: 1.056	 Accuracy train: 100.0%	 val: 81.7%
Validation loss improved from 1.0580 to 1.0560. Saving model...
Loss train: 0.600	 val: 1.054	 Accuracy train: 100.0%	 val: 81.7%
Validation loss improved from 1.0560 to 1.0539. Saving model...
Loss train: 0.599	 val: 1.052	 Accuracy train: 100.0%	 val: 82.5%
Validation loss improved from 1.0539 to 1.0521. Saving model...
Loss train: 0.597	 val: 1.050	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 1.0521 to 1.0501. Saving model...
Loss train: 0.596	 val: 1.049	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 1.0501 to 1.0488. Saving model...
Loss train: 0.595	 val: 1.046	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 1.0488 to 1.0463. Saving model...
Loss train: 0.593	 val: 1.045	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 1.0463 to 1.0446. Saving model...
Loss train: 0.592	 val: 1.043	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 1.0446 to 1.0429. Saving model...
Loss train: 0.591	 val: 1.041	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 1.0429 to 1.0411. Saving model...
Loss train: 0.590	 val: 1.039	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 1.0411 to 1.0392. Saving model...
Loss train: 0.588	 val: 1.038	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0392 to 1.0382. Saving model...
Loss train: 0.587	 val: 1.036	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0382 to 1.0358. Saving model...
Loss train: 0.586	 val: 1.035	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0358 to 1.0348. Saving model...
Loss train: 0.585	 val: 1.033	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0348 to 1.0327. Saving model...
Loss train: 0.584	 val: 1.031	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0327 to 1.0312. Saving model...
Loss train: 0.583	 val: 1.030	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0312 to 1.0301. Saving model...
Loss train: 0.582	 val: 1.028	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0301 to 1.0279. Saving model...
Loss train: 0.581	 val: 1.027	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0279 to 1.0274. Saving model...
Loss train: 0.580	 val: 1.025	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0274 to 1.0250. Saving model...
Loss train: 0.579	 val: 1.024	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0250 to 1.0239. Saving model...
Loss train: 0.578	 val: 1.023	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0239 to 1.0231. Saving model...
Loss train: 0.577	 val: 1.021	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0231 to 1.0209. Saving model...
Loss train: 0.576	 val: 1.020	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0209 to 1.0202. Saving model...
Loss train: 0.575	 val: 1.018	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0202 to 1.0181. Saving model...
Loss train: 0.574	 val: 1.017	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0181 to 1.0175. Saving model...
Loss train: 0.573	 val: 1.015	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0175 to 1.0152. Saving model...
Loss train: 0.572	 val: 1.015	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0152 to 1.0147. Saving model...
Loss train: 0.571	 val: 1.013	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0147 to 1.0127. Saving model...
Loss train: 0.570	 val: 1.012	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 1.0127 to 1.0120. Saving model...
Loss train: 0.569	 val: 1.010	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 1.0120 to 1.0101. Saving model...
Loss train: 0.568	 val: 1.010	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 1.0101 to 1.0096. Saving model...
Loss train: 0.567	 val: 1.008	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 1.0096 to 1.0078. Saving model...
Loss train: 0.567	 val: 1.007	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0078 to 1.0074. Saving model...
Loss train: 0.566	 val: 1.006	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0074 to 1.0056. Saving model...
Loss train: 0.565	 val: 1.005	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0056 to 1.0049. Saving model...
Loss train: 0.564	 val: 1.003	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0049 to 1.0030. Saving model...
Loss train: 0.563	 val: 1.003	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 1.0030 to 1.0025. Saving model...
Loss train: 0.563	 val: 1.001	 Accuracy train: 100.0%	 val: 84.2%
Validation loss improved from 1.0025 to 1.0007. Saving model...
Loss train: 0.562	 val: 1.000	 Accuracy train: 100.0%	 val: 82.5%
Validation loss improved from 1.0007 to 1.0003. Saving model...
Loss train: 0.561	 val: 0.999	 Accuracy train: 100.0%	 val: 82.5%
Validation loss improved from 1.0003 to 0.9985. Saving model...
Loss train: 0.560	 val: 0.998	 Accuracy train: 100.0%	 val: 82.5%
Validation loss improved from 0.9985 to 0.9980. Saving model...
Loss train: 0.559	 val: 0.997	 Accuracy train: 100.0%	 val: 82.5%
Validation loss improved from 0.9980 to 0.9967. Saving model...
Loss train: 0.559	 val: 0.996	 Accuracy train: 100.0%	 val: 82.5%
Validation loss improved from 0.9967 to 0.9960. Saving model...
Loss train: 0.558	 val: 0.995	 Accuracy train: 100.0%	 val: 82.5%
Validation loss improved from 0.9960 to 0.9946. Saving model...
Loss train: 0.557	 val: 0.994	 Accuracy train: 100.0%	 val: 82.5%
Validation loss improved from 0.9946 to 0.9942. Saving model...
Loss train: 0.557	 val: 0.993	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 0.9942 to 0.9928. Saving model...
Loss train: 0.556	 val: 0.992	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 0.9928 to 0.9924. Saving model...
Loss train: 0.555	 val: 0.991	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 0.9924 to 0.9910. Saving model...
Loss train: 0.555	 val: 0.991	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 0.9910 to 0.9908. Saving model...
Loss train: 0.554	 val: 0.989	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 0.9908 to 0.9894. Saving model...
Loss train: 0.553	 val: 0.989	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 0.9894 to 0.9889. Saving model...
Loss train: 0.553	 val: 0.988	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 0.9889 to 0.9880. Saving model...
Loss train: 0.552	 val: 0.988	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 0.9880 to 0.9877. Saving model...
Loss train: 0.551	 val: 0.987	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 0.9877 to 0.9866. Saving model...
Loss train: 0.551	 val: 0.986	 Accuracy train: 100.0%	 val: 83.3%
Validation loss improved from 0.9866 to 0.9859. Saving model...
Loss train: 0.550	 val: 0.986	 Accuracy train: 100.0%	 val: 83.3%
Current validation loss (0.9859327599406242) is larger than for the previous epoch (0.9858570332328479)!
Early stopping applies!
Evaluation metrics from training phase: {'train_acc': [0.108, 0.14, 0.188, 0.2, 0.22, 0.238, 0.28, 0.292, 0.302, 0.308, 0.322, 0.34, 0.35, 0.364, 0.402, 0.432, 0.436, 0.444, 0.456, 0.45, 0.454, 0.458, 0.466, 0.47, 0.486, 0.494, 0.498, 0.5, 0.514, 0.522, 0.528, 0.534, 0.538, 0.552, 0.56, 0.568, 0.572, 0.572, 0.574, 0.578, 0.584, 0.592, 0.596, 0.598, 0.61, 0.616, 0.614, 0.618, 0.62, 0.622, 0.628, 0.63, 0.634, 0.64, 0.644, 0.658, 0.66, 0.672, 0.674, 0.678, 0.682, 0.686, 0.688, 0.69, 0.692, 0.694, 0.696, 0.704, 0.708, 0.708, 0.712, 0.716, 0.724, 0.736, 0.746, 0.76, 0.764, 0.77, 0.776, 0.776, 0.778, 0.782, 0.786, 0.794, 0.8, 0.802, 0.808, 0.816, 0.818, 0.828, 0.83, 0.83, 0.83, 0.834, 0.842, 0.842, 0.848, 0.85, 0.854, 0.86, 0.862, 0.866, 0.868, 0.87, 0.874, 0.874, 0.88, 0.884, 0.884, 0.886, 0.888, 0.888, 0.89, 0.892, 0.894, 0.894, 0.894, 0.894, 0.894, 0.898, 0.898, 0.898, 0.898, 0.898, 0.9, 0.902, 0.908, 0.908, 0.91, 0.91, 0.914, 0.914, 0.918, 0.92, 0.92, 0.922, 0.926, 0.926, 0.926, 0.936, 0.938, 0.938, 0.938, 0.938, 0.938, 0.938, 0.938, 0.94, 0.94, 0.942, 0.942, 0.942, 0.946, 0.946, 0.948, 0.948, 0.95, 0.952, 0.956, 0.956, 0.956, 0.956, 0.956, 0.964, 0.964, 0.97, 0.972, 0.972, 0.972, 0.972, 0.972, 0.974, 0.98, 0.98, 0.986, 0.988, 0.988, 0.988, 0.988, 0.99, 0.99, 0.99, 0.99, 0.99, 0.994, 0.994, 0.994, 0.994, 0.996, 0.996, 0.996, 0.996, 0.996, 0.996, 0.996, 0.996, 0.996, 0.996, 0.996, 0.998, 0.998, 0.998, 0.998, 0.998, 0.998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_acc': [0.11666666666666667, 0.16666666666666666, 0.16666666666666666, 0.18333333333333332, 0.18333333333333332, 0.19166666666666668, 0.19166666666666668, 0.19166666666666668, 0.19166666666666668, 0.2, 0.20833333333333334, 0.23333333333333334, 0.225, 0.25833333333333336, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.275, 0.2833333333333333, 0.275, 0.2833333333333333, 0.2833333333333333, 0.31666666666666665, 0.3333333333333333, 0.35, 0.35, 0.3416666666666667, 0.3333333333333333, 0.3416666666666667, 0.35, 0.36666666666666664, 0.38333333333333336, 0.38333333333333336, 0.38333333333333336, 0.375, 0.375, 0.38333333333333336, 0.38333333333333336, 0.375, 0.375, 0.39166666666666666, 0.39166666666666666, 0.39166666666666666, 0.4, 0.4, 0.4, 0.4083333333333333, 0.4083333333333333, 0.4166666666666667, 0.425, 0.425, 0.43333333333333335, 0.43333333333333335, 0.45, 0.4583333333333333, 0.45, 0.4583333333333333, 0.4666666666666667, 0.475, 0.475, 0.475, 0.48333333333333334, 0.48333333333333334, 0.48333333333333334, 0.48333333333333334, 0.48333333333333334, 0.48333333333333334, 0.48333333333333334, 0.48333333333333334, 0.49166666666666664, 0.49166666666666664, 0.5083333333333333, 0.5166666666666667, 0.5083333333333333, 0.5166666666666667, 0.5166666666666667, 0.5166666666666667, 0.525, 0.525, 0.5333333333333333, 0.5333333333333333, 0.5333333333333333, 0.5333333333333333, 0.5416666666666666, 0.5416666666666666, 0.5416666666666666, 0.5666666666666667, 0.5666666666666667, 0.5666666666666667, 0.5666666666666667, 0.5666666666666667, 0.575, 0.575, 0.575, 0.575, 0.575, 0.575, 0.575, 0.575, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5833333333333334, 0.5916666666666667, 0.5916666666666667, 0.5916666666666667, 0.6083333333333333, 0.6083333333333333, 0.6166666666666667, 0.6166666666666667, 0.625, 0.6333333333333333, 0.6416666666666667, 0.6416666666666667, 0.6416666666666667, 0.6416666666666667, 0.6416666666666667, 0.6416666666666667, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.6666666666666666, 0.675, 0.675, 0.675, 0.675, 0.675, 0.6666666666666666, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.6833333333333333, 0.6916666666666667, 0.7, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.7166666666666667, 0.7166666666666667, 0.725, 0.725, 0.7333333333333333, 0.7333333333333333, 0.7416666666666667, 0.7416666666666667, 0.7416666666666667, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7583333333333333, 0.7583333333333333, 0.7583333333333333, 0.7583333333333333, 0.7583333333333333, 0.7583333333333333, 0.7583333333333333, 0.7583333333333333, 0.7583333333333333, 0.7583333333333333, 0.7666666666666667, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7833333333333333, 0.7916666666666666, 0.8, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8083333333333333, 0.8166666666666667, 0.8166666666666667, 0.8166666666666667, 0.825, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8416666666666667, 0.8333333333333334, 0.8416666666666667, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334], 'train_loss': [np.float64(2.3097459621429444), np.float64(2.2891186027526857), np.float64(2.2703352184295653), np.float64(2.254612006187439), np.float64(2.239242051124573), np.float64(2.223373517036438), np.float64(2.206864594697952), np.float64(2.1924332835674285), np.float64(2.175914643287659), np.float64(2.1611897673606872), np.float64(2.144429570198059), np.float64(2.1280711605548857), np.float64(2.112293142080307), np.float64(2.0971471729278566), np.float64(2.079301504135132), np.float64(2.0629392125606536), np.float64(2.0475021533966062), np.float64(2.033010411977768), np.float64(2.0179852714538575), np.float64(2.003472974061966), np.float64(1.9896559250354766), np.float64(1.9750787551403046), np.float64(1.9608540709018707), np.float64(1.946543312072754), np.float64(1.9322691705226898), np.float64(1.9187578048706055), np.float64(1.9045459282398225), np.float64(1.8906177308559418), np.float64(1.8772432317733765), np.float64(1.863918730020523), np.float64(1.850883658528328), np.float64(1.8378708013296128), np.float64(1.8254439128637314), np.float64(1.8126650171279908), np.float64(1.79983729493618), np.float64(1.7870887259244919), np.float64(1.7739770241975785), np.float64(1.7617019999027252), np.float64(1.748328452348709), np.float64(1.736881880044937), np.float64(1.7242274552583694), np.float64(1.711551377415657), np.float64(1.699336420059204), np.float64(1.6875591512918473), np.float64(1.6753673030138017), np.float64(1.6634381917715073), np.float64(1.6515276601314546), np.float64(1.6396228158473969), np.float64(1.6278665170669555), np.float64(1.616166697382927), np.float64(1.60461112344265), np.float64(1.5930551918745042), np.float64(1.5815382142066956), np.float64(1.5698922240734101), np.float64(1.5583937804698944), np.float64(1.546804444551468), np.float64(1.5349724023342133), np.float64(1.523362088084221), np.float64(1.5119957470893859), np.float64(1.5006289576292038), np.float64(1.4893141057491301), np.float64(1.4782216470241547), np.float64(1.4671004561185836), np.float64(1.4560987806320191), np.float64(1.4452090077400208), np.float64(1.4343891125917434), np.float64(1.4234772833585738), np.float64(1.4127129539251329), np.float64(1.4022390307188035), np.float64(1.391665623188019), np.float64(1.3811258834600448), np.float64(1.3706587033271789), np.float64(1.3602959271669388), np.float64(1.3500258870124817), np.float64(1.339877611041069), np.float64(1.3297844376564025), np.float64(1.3198018003702163), np.float64(1.309984317779541), np.float64(1.2999607671499251), np.float64(1.2901176013946534), np.float64(1.2805265890359878), np.float64(1.2709454584121704), np.float64(1.261413587450981), np.float64(1.2519601639509201), np.float64(1.2425246336460114), np.float64(1.2334416275024414), np.float64(1.2240639955997468), np.float64(1.2151681537628174), np.float64(1.2059428803920746), np.float64(1.1972215749025346), np.float64(1.1881950191259385), np.float64(1.1796435500383378), np.float64(1.1707894461154937), np.float64(1.1624249210357667), np.float64(1.1537241686582564), np.float64(1.145518020272255), np.float64(1.1370243666172029), np.float64(1.1289939767122268), np.float64(1.1207091665267945), np.float64(1.1127096799612046), np.float64(1.1044627075195312), np.float64(1.0964920455217362), np.float64(1.0888638453483581), np.float64(1.0807839077711106), np.float64(1.0732072104215622), np.float64(1.0656401107311249), np.float64(1.0582153041362763), np.float64(1.0505555905103683), np.float64(1.0431972055435181), np.float64(1.0358844513893128), np.float64(1.028657851457596), np.float64(1.0213985430002213), np.float64(1.014236313343048), np.float64(1.0071830760240554), np.float64(1.0001066873073579), np.float64(0.9932142893075943), np.float64(0.9864124468564988), np.float64(0.979629475235939), np.float64(0.9729767774343491), np.float64(0.9663457210063935), np.float64(0.959851744890213), np.float64(0.9534023141860962), np.float64(0.9471065526008606), np.float64(0.9407917383909226), np.float64(0.9345985745191574), np.float64(0.928450246810913), np.float64(0.9224139281511307), np.float64(0.916442309975624), np.float64(0.9105373032093048), np.float64(0.9046902076005936), np.float64(0.8989418586492538), np.float64(0.893301393866539), np.float64(0.8876720644235611), np.float64(0.8821563612222671), np.float64(0.8766465504169464), np.float64(0.8712446293830871), np.float64(0.8657533476352691), np.float64(0.8609757615327835), np.float64(0.8553770738840103), np.float64(0.8502545231580734), np.float64(0.8452053505182267), np.float64(0.840221459031105), np.float64(0.8352588211297989), np.float64(0.8304045286178589), np.float64(0.8255952082872391), np.float64(0.8209057633876801), np.float64(0.8162145369052887), np.float64(0.8116602160930634), np.float64(0.8071161810159683), np.float64(0.8026657804250718), np.float64(0.7982538509368896), np.float64(0.793913184762001), np.float64(0.7896160120964051), np.float64(0.7853483066558838), np.float64(0.7811763383150101), np.float64(0.7770543454885482), np.float64(0.773012206196785), np.float64(0.7689917805194855), np.float64(0.7650366457700729), np.float64(0.7611675856113433), np.float64(0.7573434282541275), np.float64(0.7535004997253418), np.float64(0.7497889697551727), np.float64(0.746111578822136), np.float64(0.7424571399688721), np.float64(0.738906191110611), np.float64(0.7353768066167832), np.float64(0.7318781143426896), np.float64(0.7284575809240341), np.float64(0.725084751367569), np.float64(0.7217317779064178), np.float64(0.7184412834644318), np.float64(0.7152159706354141), np.float64(0.7120456137657165), np.float64(0.7088765439987182), np.float64(0.7057792531251907), np.float64(0.7027284457683564), np.float64(0.6997192982435226), np.float64(0.6967820773124694), np.float64(0.6938710229396821), np.float64(0.6909818619489669), np.float64(0.6881645922660827), np.float64(0.6854065639972686), np.float64(0.6826702443361282), np.float64(0.6799679600000381), np.float64(0.6773318824768066), np.float64(0.6747294239997864), np.float64(0.6721551519632339), np.float64(0.6696432459354401), np.float64(0.6671517560482025), np.float64(0.6647128415107727), np.float64(0.6623113561868668), np.float64(0.659948323726654), np.float64(0.6576074225902557), np.float64(0.6553077481985092), np.float64(0.6530301498174668), np.float64(0.6507912839651108), np.float64(0.6485862225294113), np.float64(0.6464368267059326), np.float64(0.644308013677597), np.float64(0.642218310713768), np.float64(0.6401752024888993), np.float64(0.6381507402658463), np.float64(0.6361659244298935), np.float64(0.6342361900806427), np.float64(0.6323116841316223), np.float64(0.6304314079284667), np.float64(0.6285655391216278), np.float64(0.6267358076572418), np.float64(0.6249327985048294), np.float64(0.6231815186738968), np.float64(0.6214271292686463), np.float64(0.6197062127590179), np.float64(0.6180200531482697), np.float64(0.6163686563968659), np.float64(0.6147453988790512), np.float64(0.6131285346746445), np.float64(0.611579048871994), np.float64(0.610021629691124), np.float64(0.6085060839653015), np.float64(0.6070109232664108), np.float64(0.6055378693342209), np.float64(0.6040936313867569), np.float64(0.6026931723356247), np.float64(0.6012725112438202), np.float64(0.5998895322084427), np.float64(0.5985551996231079), np.float64(0.5972080026865005), np.float64(0.5959049459695817), np.float64(0.5945907982587815), np.float64(0.5933551567792893), np.float64(0.5921012551784516), np.float64(0.5908739417791367), np.float64(0.5896781483888626), np.float64(0.5884935433864593), np.float64(0.5873125764131546), np.float64(0.5861992492675782), np.float64(0.5850380370616913), np.float64(0.5839445296525955), np.float64(0.5828723390102386), np.float64(0.5817597082853317), np.float64(0.5807090096473694), np.float64(0.5796325271129608), np.float64(0.5786231209039688), np.float64(0.5776110649108886), np.float64(0.5765878028869629), np.float64(0.5756410652399063), np.float64(0.5746368607282638), np.float64(0.5737052990198135), np.float64(0.5727603534460067), np.float64(0.571841801404953), np.float64(0.5709106822013855), np.float64(0.5700352025032044), np.float64(0.5691383538246155), np.float64(0.568274318099022), np.float64(0.5673980641365052), np.float64(0.5665785077810287), np.float64(0.5657278939485549), np.float64(0.5649276235103607), np.float64(0.5640904132127762), np.float64(0.5633139350414276), np.float64(0.5625068203210831), np.float64(0.561755068063736), np.float64(0.5609662826061249), np.float64(0.5602586098909378), np.float64(0.5594891110658645), np.float64(0.5587841901779175), np.float64(0.5580385860204696), np.float64(0.5573544199466706), np.float64(0.5566292679309846), np.float64(0.5559632093906403), np.float64(0.5552631392478943), np.float64(0.5545985809564591), np.float64(0.5539348084926605), np.float64(0.5533032015562057), np.float64(0.5526334199905395), np.float64(0.5520295339822769), np.float64(0.5513829567432403), np.float64(0.5507729512453079), np.float64(0.5501699838638305)], 'val_loss': [np.float64(2.2945289413134256), np.float64(2.2836279471715293), np.float64(2.2738361914952594), np.float64(2.2647506018479664), np.float64(2.2547971924146015), np.float64(2.245742909113566), np.float64(2.235271433989207), np.float64(2.2265935828288397), np.float64(2.2159803142150243), np.float64(2.2062802096207936), np.float64(2.1961266040802), np.float64(2.186622448762258), np.float64(2.178328964114189), np.float64(2.169342741370201), np.float64(2.1594379127025602), np.float64(2.1508809169133505), np.float64(2.142741411924362), np.float64(2.1367929985125858), np.float64(2.1293202499548594), np.float64(2.1214397112528482), np.float64(2.114094306031863), np.float64(2.1069758504629137), np.float64(2.0999399085839587), np.float64(2.0929479440053305), np.float64(2.0856292635202407), np.float64(2.078463939825694), np.float64(2.0710786789655686), np.float64(2.0632388959328334), np.float64(2.0558635582526525), np.float64(2.0485964914162955), np.float64(2.041218071182569), np.float64(2.0338974754015604), np.float64(2.026378269990285), np.float64(2.019643611709277), np.float64(2.012412733832995), np.float64(2.0049278259277346), np.float64(1.9972572416067123), np.float64(1.9899779697259268), np.float64(1.9835927193363507), np.float64(1.9762838115294774), np.float64(1.9684395477175713), np.float64(1.9607827057441076), np.float64(1.9531123851736387), np.float64(1.945251627266407), np.float64(1.9372558971246083), np.float64(1.9294496620694797), np.float64(1.9218214134375253), np.float64(1.9140882382790247), np.float64(1.9066829239328702), np.float64(1.898938962817192), np.float64(1.8914225533604623), np.float64(1.883643777668476), np.float64(1.8759783724943797), np.float64(1.86812827338775), np.float64(1.8602855076392493), np.float64(1.8535523205995559), np.float64(1.8457938442627588), np.float64(1.837826778491338), np.float64(1.829848297437032), np.float64(1.8218971888224285), np.float64(1.8136749058961867), np.float64(1.8062246277928353), np.float64(1.7987569525837899), np.float64(1.7911112993955611), np.float64(1.7837095096707345), np.float64(1.7760670309265454), np.float64(1.7697670737902322), np.float64(1.7612561230858168), np.float64(1.7539175416032473), np.float64(1.7462800443172455), np.float64(1.7390470628937085), np.float64(1.7315973644455274), np.float64(1.7245627517501514), np.float64(1.7172975793480874), np.float64(1.7104243417580922), np.float64(1.7032198205590248), np.float64(1.6978445520003638), np.float64(1.6893457099795341), np.float64(1.6825858980417252), np.float64(1.675638680656751), np.float64(1.6689630880951882), np.float64(1.662355907758077), np.float64(1.6556250045696894), np.float64(1.6490567793448767), np.float64(1.64240047087272), np.float64(1.6360429634650548), np.float64(1.6295232440034548), np.float64(1.6232294524709383), np.float64(1.6167498926321666), np.float64(1.6105002408226332), np.float64(1.6042447174588839), np.float64(1.598113127052784), np.float64(1.5916771441698074), np.float64(1.5855252901713053), np.float64(1.5793437163035076), np.float64(1.5734389593203864), np.float64(1.5674455339709918), np.float64(1.561960470676422), np.float64(1.5560901532570521), np.float64(1.5502084935704867), np.float64(1.5449109807610513), np.float64(1.5406057685613632), np.float64(1.5339792773127556), np.float64(1.5303353856007258), np.float64(1.5253884012500445), np.float64(1.5197806864976884), np.float64(1.5144701908032099), np.float64(1.5089024384816487), np.float64(1.5034555440147719), np.float64(1.4978515605131786), np.float64(1.492555063466231), np.float64(1.4871595243612925), np.float64(1.4806705762942631), np.float64(1.4743887747327487), np.float64(1.468277599910895), np.float64(1.4627813532948495), np.float64(1.4574193333586056), np.float64(1.4522823398311933), np.float64(1.447203566133976), np.float64(1.4422333667675653), np.float64(1.4372541457414627), np.float64(1.4322685907284418), np.float64(1.4276580090324085), np.float64(1.4230939005812009), np.float64(1.4181804224848746), np.float64(1.4135272726416588), np.float64(1.4088148822387059), np.float64(1.4040963783860207), np.float64(1.3993531549970308), np.float64(1.3947262197732926), np.float64(1.3901037777463594), np.float64(1.3854017009337742), np.float64(1.380448167026043), np.float64(1.3758317743738493), np.float64(1.371043494840463), np.float64(1.366454903781414), np.float64(1.3604686732093494), np.float64(1.3568701361616453), np.float64(1.3524940739075342), np.float64(1.3479103262225787), np.float64(1.3434331769744554), np.float64(1.3388838216662406), np.float64(1.33421614219745), np.float64(1.3295648142695426), np.float64(1.3250912388165792), np.float64(1.3206114172935486), np.float64(1.3163075024882953), np.float64(1.3117999146382013), np.float64(1.3076510017116865), np.float64(1.3032315755883852), np.float64(1.2988901938001314), np.float64(1.2944223900636038), np.float64(1.29006207883358), np.float64(1.2857662662863731), np.float64(1.2815047765771548), np.float64(1.2774495189388593), np.float64(1.273140122493108), np.float64(1.2690103073914847), np.float64(1.264778196811676), np.float64(1.2606085643172265), np.float64(1.2564590970675151), np.float64(1.2523322840531668), np.float64(1.2483330150445302), np.float64(1.2440848747889202), np.float64(1.2399863600730896), np.float64(1.2358650162816047), np.float64(1.2318145910898843), np.float64(1.2280311013261478), np.float64(1.223843514919281), np.float64(1.2198324804504712), np.float64(1.2160291468103728), np.float64(1.2121276825666427), np.float64(1.2084111938873927), np.float64(1.2044718533754348), np.float64(1.200780130426089), np.float64(1.1971150065461795), np.float64(1.1933882301052412), np.float64(1.1898159474134444), np.float64(1.1860179021954536), np.float64(1.1824470579624176), np.float64(1.178752268354098), np.float64(1.1755014325181643), np.float64(1.1718722105026245), np.float64(1.1685044611493747), np.float64(1.1652070164680481), np.float64(1.1618441020449002), np.float64(1.158405206600825), np.float64(1.1550916517774263), np.float64(1.1519084716836612), np.float64(1.148598268131415), np.float64(1.1455274457732836), np.float64(1.1421400899688403), np.float64(1.1390291690826415), np.float64(1.1359358981251717), np.float64(1.1328827554980914), np.float64(1.1296986217300098), np.float64(1.1267337371905646), np.float64(1.1236046304305394), np.float64(1.1205465465784072), np.float64(1.1177527169386545), np.float64(1.11485236287117), np.float64(1.1118828281760216), np.float64(1.109072591861089), np.float64(1.1063195581237475), np.float64(1.1037055750687916), np.float64(1.1008970747391382), np.float64(1.0982536916931471), np.float64(1.0956381201744079), np.float64(1.0927599360545477), np.float64(1.0902609720826149), np.float64(1.0875316912929216), np.float64(1.0851111819346746), np.float64(1.0827566196521123), np.float64(1.0802304978171984), np.float64(1.0779415021340053), np.float64(1.0754542748133342), np.float64(1.073143660525481), np.float64(1.0709132303794224), np.float64(1.0686756213506063), np.float64(1.066367440422376), np.float64(1.0642874201138814), np.float64(1.0622413019339243), np.float64(1.0599123840530713), np.float64(1.0579772755503654), np.float64(1.0559730306267738), np.float64(1.0539486368497213), np.float64(1.0521238446235657), np.float64(1.0500956724087398), np.float64(1.0487802376349766), np.float64(1.0463390032450357), np.float64(1.0445531070232392), np.float64(1.0428638180096945), np.float64(1.0410557876030604), np.float64(1.039197133978208), np.float64(1.0382050976157189), np.float64(1.0358026166756948), np.float64(1.0348156546552976), np.float64(1.0327341690659524), np.float64(1.0311991254488626), np.float64(1.0301390156149863), np.float64(1.0279323478539786), np.float64(1.0273619025945664), np.float64(1.0250487744808197), np.float64(1.0239048595229785), np.float64(1.0230613127350807), np.float64(1.0208556294441222), np.float64(1.020163635412852), np.float64(1.018096922338009), np.float64(1.017498536904653), np.float64(1.0152348433931668), np.float64(1.014707773923874), np.float64(1.0127417787909507), np.float64(1.012002357840538), np.float64(1.0101285437742868), np.float64(1.0096086487174034), np.float64(1.0077855010827383), np.float64(1.0073516448338826), np.float64(1.0055544858177503), np.float64(1.0048911174138386), np.float64(1.0030092184742292), np.float64(1.0025035659472148), np.float64(1.0006807362039885), np.float64(1.000288292268912), np.float64(0.9985062951842943), np.float64(0.9980472162365913), np.float64(0.9966828828056653), np.float64(0.9960333988070488), np.float64(0.9945775633056958), np.float64(0.9941622818509738), np.float64(0.9928131371736526), np.float64(0.9923895602424939), np.float64(0.9910069763660431), np.float64(0.9907814671595891), np.float64(0.9893904651204745), np.float64(0.9888943870862325), np.float64(0.9879683295885722), np.float64(0.9877187023560207), np.float64(0.9865965063373248), np.float64(0.9858570332328479), np.float64(0.9859327599406242)], 'n_epochs': 280}
Evaluation metrics on the test set: {'Test loss': np.float64(1.0557620783646902), 'Test accuracy': 0.8416666666666667}

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 26551644: <single_frame_cnn> in cluster <dcc> Done

Job <single_frame_cnn> was submitted from host <hpclogin1> by user <s254120> in cluster <dcc> at Sat Oct 18 17:25:27 2025
Job was executed on host(s) <4*n-62-18-13>, in queue <c02516>, as user <s254120> in cluster <dcc> at Sat Oct 18 17:25:27 2025
</zhome/70/1/224436> was used as the home directory.
</zhome/70/1/224436/02516-video-classification> was used as the working directory.
Started at Sat Oct 18 17:25:27 2025
Terminated at Sat Oct 18 18:57:57 2025
Results reported at Sat Oct 18 18:57:57 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
### ------------- specify queue name ----------------
#BSUB -q c02516
### ------------- specify gpu request----------------
#BSUB -gpu "num=1:mode=exclusive_process"
### ------------- specify job name ----------------
#BSUB -J single_frame_cnn
### ------------- specify number of cores ----------------
#BSUB -n 4
#BSUB -R "span[hosts=1]"

#BSUB -R "rusage[mem=20GB]"

### ------------- specify wall-clock time (max allowed is 12:00)---------------- 
#BSUB -W 08:00

#BSUB -o OUTPUT_FILE%J.out
#BSUB -e OUTPUT_FILE%J.err

source venv_proj2/bin/activate
python single_frame.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   5317.81 sec.
    Max Memory :                                 1169 MB
    Average Memory :                             1140.71 MB
    Total Requested Memory :                     81920.00 MB
    Delta Memory :                               80751.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                8
    Run time :                                   5582 sec.
    Turnaround time :                            5550 sec.

The output (if any) is above this job summary.



PS:

Read file <OUTPUT_FILE26551644.err> for stderr output of this job.

